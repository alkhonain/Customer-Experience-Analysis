# **Project**

The competitiveness between coffee shops is increasing exponentially, and customer reviews usually determines the success of each café; so we decided to offer help by making a sentiment analysis of customer reviews, and getting each Café's needs based on multiple factors.
" NEED TO BE EDITED"

## Dataset

Our dataset is scraped from yelp.com containing nearly 7,000 reviews, about coffee shops in Austin and Texas. The attributes are replaced, and sentiments around attributes.
" NEED TO BE CHANGED "

The columns we used:

-   Coffee shop name

-   Review text

-   Review Score

### Import

```{r}
install.packages("SnowballC")
```

```{r}
library(ggplot2)
library(mlbench)
library(tidyverse)
library(psych)
library(dplyr)
library(tm)
library(tokenizers)
library(SnowballC)
```

```{r}
df <- read_csv("Dataset/ratings_and_sentiments.csv")
# Rows: 7621 Columns: 20  
df <-as.data.frame(df)
print(class(df))
head(df)

```

```{r}
view(df)
describe(df) # the start mean the value is object otherwise its numbercal
```

Removing missing values and checking where they occurred.

```{r}
sum(is.na(df)) # Checks whether the DS contains any missing values
sum(is.na(df$vibe_sent)) # vibe_sent column contains most of the missing values; so we will remove it.
df = select(df, -vibe_sent)
df <- na.omit(df)
sum(is.na(df)) 
```

Remove any Coffee Shop with less than 20 reviews.

```{r}
table(df['coffee_shop_name'])

df <- df[df$coffee_shop_name != 'Sister Coffee',]
df <- df[df$coffee_shop_name != 'Kowabunga Coffee',]
df <- df[df$coffee_shop_name != 'The Marvelous Vintage Tea Party Co.',]
df <- df[df$coffee_shop_name != 'Lola Savannah Coffee Downtown',]
```

```{r}
nrow(df)
```

Processing the reviews by cleaning them from any numbers, links, spaces, punctuation etc...

```{r}
# Get the text column
text <- df$review_text
# Removing mentions, URLs, Emojis, numbers, punctuation, etc.
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
# Remove spaces and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
# Set the text to lowercase
text <- tolower(text)
```

Using NLP techniques to remove stop words, tokenize, etc...

```{r}
documents = c("She had toast for breakfast",
              "The coffee this morning was excellent", 
              "For lunch let's all have pancakes", 
              "Later in the day, there will be more talks", 
              "The talks on the first day were great", 
              "The second day should have good presentations too")
documents <- Corpus(VectorSource(documents))
documents = tm_map(documents, content_transformer(tolower))
documents = tm_map(documents, removePunctuation)
documents = tm_map(documents, stripWhitespace)
documents = tm_map(documents, removeWords, stopwords("english"))
documents

documents[[1]]$content
documents[[2]]$content
documents[[3]]$content
documents[[4]]$content
documents[[5]]$content
documents[[6]]$content
```

Above is a sample of what kind of removal will be applied to the text, so if we implement that on the review_text it will be as shown below: " NEED TO BE EDITED LATER "

```{r}
view(text)
```

```{r}
documents <- Corpus(VectorSource(text))
documents = tm_map(documents, content_transformer(tolower))
documents = tm_map(documents, removePunctuation)
documents = tm_map(documents, removeWords, stopwords("english"))
documents

documents[[1]]$content
documents[[2]]$content
documents[[3]]$content
documents[[4]]$content
documents[[5]]$content
documents[[6]]$content
```

The text is better after we normalized it, but some words are not in their proper use cases; so we will check every word in the English language dictionary, and see whether it's an English word or not.

```{r}
documents[[1]]$content
tokenize_words(documents[[1]]$content)
```

The idea above shows how it splits each word individually, *Tokenize* the next step is stemming and lemmatization

Stemming Words - After tokenization, we need to analyze each word by breaking it down to it's root (stemming) and conjugation affix. " NEED TO BE EDIT LATER "

```{r}
values <- documents$content
print("Before Stemming")
print(values[1])
# values <- tm_map(values, stemDocument)
print("After Stemming")
print(values[1])

```