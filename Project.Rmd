# **Project**

The competitiveness between coffee shops is increasing exponentially, and customer reviews usually determines the success of each café; so we decided to offer help by making a sentiment analysis of customer reviews, and getting each Café's needs based on multiple factors. " NEED TO BE EDITED"

## Dataset

Our dataset is scraped from yelp.com containing nearly 7,000 reviews, about coffee shops in Austin and Texas. The attributes are replaced, and sentiments around attributes. " NEED TO BE CHANGED "

The columns we used:

-   Coffee shop name

-   Review text

-   Review Score

### Import

```{r}
install.packages("wordcloud")
```

```{r}
library(ggplot2)
library(mlbench)
library(tidyverse)
library(psych)
library(dplyr)
library(tm)
library(tokenizers)
library(syuzhet)
library(wordcloud)
library(wordcloud2)
```

```{r}
df <- read_csv("Dataset/ratings_and_sentiments.csv")
# Rows: 7621 Columns: 20  
df <-as.data.frame(df)
print(class(df))
head(df)

```

```{r}
view(df)
describe(df) # the start mean the value is object otherwise its numbercal
```

Removing missing values and checking where they occurred.

```{r}
sum(is.na(df)) # Checks whether the DS contains any missing values
sum(is.na(df$vibe_sent)) # vibe_sent column contains most of the missing values; so we will remove it.
df <- select(df, -vibe_sent)
df <- na.omit(df)
sum(is.na(df)) 
```

```{r}
df <- select(df, c(1,2,4,6))
view(df)
```

Remove any Coffee Shop with less than 20 reviews.

```{r}
table(df['coffee_shop_name'])

df <- df[df$coffee_shop_name != 'Sister Coffee',]
df <- df[df$coffee_shop_name != 'Kowabunga Coffee',]
df <- df[df$coffee_shop_name != 'The Marvelous Vintage Tea Party Co.',]
df <- df[df$coffee_shop_name != 'Lola Savannah Coffee Downtown',]
```

```{r}
nrow(df)
```

```{r}
sum(df$bool_HIGH)
```

6094 was giving high review results which is above 3, the plot below will shows the result in more details

```{r}
ggplot(df, aes(x = as.factor(bool_HIGH))) + 
  geom_bar(fill = "skyblue") +
  labs(x = "Satisfaction", y = "Count") +
  scale_x_discrete(labels = c("Low", "High")) +
  coord_flip() +
  theme_bw()
```

Processing the reviews by cleaning them from![]() any numbers, links, spaces, punctuation etc...

```{r}
# Get the text column
text <- df$review_text
# Removing mentions, URLs, Emojis, numbers, punctuation, etc.
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
# Remove spaces and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
# Set the text to lowercase
text <- tolower(text)
```

Using NLP techniques to remove stop words, tokenize, etc...

```{r}
documents = c("She had toast for breakfast",
              "The coffee this morning was excellent", 
              "For lunch let's all have pancakes", 
              "Later in the day, there will be more talks", 
              "The talks on the first day were great", 
              "The second day should have good presentations too")
documents <- Corpus(VectorSource(documents))
documents = tm_map(documents, content_transformer(tolower))
documents = tm_map(documents, removePunctuation)
documents = tm_map(documents, stripWhitespace)
documents = tm_map(documents, removeWords, stopwords("english"))
documents

documents[[1]]$content
documents[[2]]$content
documents[[3]]$content
documents[[4]]$content
documents[[5]]$content
documents[[6]]$content
```

Above is a sample of what kind of removal will be applied to the text, so if we implement that on the review_text it will be as shown below:

" NEED TO BE EDITED LATER "

it's just example to show the concept of removing stop words only

```{r}
view(text)
```

```{r}
documents <- Corpus(VectorSource(text))
documents = tm_map(documents, content_transformer(tolower))
documents = tm_map(documents, removePunctuation)
documents = tm_map(documents, removeWords, stopwords("english"))
documents

documents[[1]]$content
documents[[2]]$content
documents[[3]]$content
documents[[4]]$content
documents[[5]]$content
documents[[6]]$content
```

The text above is cleaned from stopping words etc..

```{r}
get_nrc_sentiment('happy')
get_nrc_sentiment('excitment')

reviews <- documents$content
values <- get_nrc_sentiment(reviews) 
```

```{r}
reviews <- documents$content
review_sentiment <- cbind(reviews, values)
head(review_sentiment)
```

```{r}
barplot(colSums(values), col = rainbow(11), ylab = "Count", main = "Sentiment Scores for Coffee Reviews")
```

```{r}
sent <- data.frame(colSums(values), row.names = 1:10)
sent$Sentiment <- colnames(values)

ggplot(sent, aes(x = Sentiment, y = colSums.values.)) +
    geom_bar(stat = 'identity', position = 'dodge')
```

Take the reviews text and corpus of it

```{r}
#Create a vector containing only the text
texts <- review_sentiment$reviews

# Create a corpus  
docs <- Corpus(VectorSource(texts))

docs <- docs %>%
  tm_map(removeNumbers) %>%
  tm_map(removePunctuation) %>%
  tm_map(stripWhitespace)
docs <- tm_map(docs, content_transformer(tolower))
docs <- tm_map(docs, removeWords, stopwords("english"))
```

Create data frame that contains each word and the number of appearance in the text

```{r}
dtm <- TermDocumentMatrix(docs) 
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df_rev <- data.frame(word = names(words),freq=words)
```

Generate the word cloud

```{r}
set.seed(1234) # for reproducibility 

wordcloud(words = df_rev$word, freq = df_rev$freq, min.freq = 1,           max.words=200, random.order=FALSE, rot.per=0.35,            colors=brewer.pal(8, "Dark2"))
```

```{r}

wordcloud2(data=df_rev, size=1.6, color='random-dark')
```

Create a function that will now

## Group By Coffee Shop Name

```{r}
df_total <- cbind(df$coffee_shop_name,reviews,values, df$num_rating, df$bool_HIGH)
```

```{r}
df_grouped <- df_total %>% group_by(`df$coffee_shop_name`)
```

## Model

```{r}
text_df <- tibble(text_review = df_total$reviews)
text_df['id'] <- seq(1:7566)
library(hunspell)
library(SnowballC)
library(xtable)
library(knitr)
library(kableExtra)
library(reshape2)
library(ggplot2)
library(tidytext)
library(stringr) 
library(tidyr)   
library(wordcloud)
text_df <- text_df %>%  unnest_tokens(word, text_review)
```

```{r}
getStemLanguages() %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

```{r}
text_df$word <- wordStem(text_df$word,  language = "english")
```

```{r}
head(table(text_df$word)) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

```{r}
data(stop_words)
text_df <- text_df %>% 
  anti_join(stop_words, "word")
```

```{r}
xtable(head(text_df %>% 
       count(word, sort = TRUE))) %>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

```{r}
text_df %>% 
  count(word, sort = TRUE) %>% 
  filter(n > 1000) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + 
  geom_col() + 
  xlab(NULL) + 
  coord_flip()
```

```{r}
Sentiment_Analysis <- text_df %>% 
  inner_join(get_sentiments("bing"), "word") %>% 
  count(id, sentiment) %>% 
  spread(sentiment, n, fill = 0) %>% 
  mutate(sentiment = positive - negative)
```

One way to analyze the sentiment of a text is to consider the text as a combination of its individual word, and the sentiment content of the whole text as the sum of the sentiment content of the individual words.

```{r}
head(Sentiment_Analysis)%>%
  kable() %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"), full_width = F)
```

if the sentiment is positive value then it's positive and Negative otherwise.

Most Common Positive and Negative Words. Now we can analyze word count that contribute to each sentiment.

```{r}
Sentiment_Analysis_Word_Count <- text_df %>% 
  inner_join(get_sentiments("bing"), "word") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()
```

Most Common Positive and Negative Words. Now we can analyze word count that contribute to each sentiment.

```{r}
Sentiment_Analysis_Word_Count <- text_df %>% 
  inner_join(get_sentiments("bing"), "word") %>% 
  count(word, sentiment, sort = TRUE) %>% 
  ungroup()
```

```{r}
Sentiment_Analysis_Word_Count %>% 
  group_by(sentiment) %>% 
  top_n(12, n) %>%
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) + 
  geom_col(show.legend = FALSE) + 
  facet_wrap(~sentiment, scales = "free_y") + 
  labs(y = "Contribution to Sentiment", x = NULL) + 
  coord_flip()
```

Word Clouds

```{r}
text_df %>% 
  anti_join(stop_words, "word") %>%
  count(word) %>% 
  with(wordcloud(word, n, max.words = 100))
```

```{r}
text_df %>% 
  inner_join(get_sentiments("bing"), "word") %>%
  count(word, sentiment, sort = TRUE) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>% 
  comparison.cloud(colors = c("gray20", "gray80"), max.words = 100)
```

```{r}
term_frequency_review <- text_df %>% count(word, sort = TRUE)
term_frequency_review$total_words <- as.numeric(term_frequency_review %>% summarize(total = sum(n)))
term_frequency_review$document <- as.character("Review")
term_frequency_review <- term_frequency_review %>% 
  bind_tf_idf(word, document, n)
```

```{r}
term_frequency_review %>% 
  arrange(desc(tf)) %>% 
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(document) %>% 
  top_n(15, tf) %>% 
  ungroup() %>% 
  ggplot(aes(word, tf, fill = document)) + 
  geom_col(show.legend = FALSE) + 
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap(~document, ncol = 2, scales = "free") + 
  coord_flip()
```

From the graph above it shows TD-IDF **term frequency-inverse document frequency,** Shows that the

-   coffee

-   food

-   service

-   seat

-   check

-   time

-   drink

-   park

    are the highest Priority from the full reviews.

Convert text into list of number using Tidying dfm

```{r}
library(tidytext)

inaug_dfm <- df_total$reviews %>%
  quanteda::tokens() %>%
  quanteda::dfm(verbose = FALSE)
inaug_dfm
```

```{r}
inaug_td <- tidy(inaug_dfm)
inaug_td
```

```{r}
inaug_tf_idf <- inaug_td %>%
  bind_tf_idf(term, document, count) %>%
  arrange(desc(tf_idf))

inaug_tf_idf
```

```{r}
print(nrow(inaug_dfm))
print(ncol(df_final))
print(class(inaug_dfm))
print(nrow(text_df))
print(nrow(Sentiment_Analysis))
```

Function to calculate if the text positive or negitive

```{r}
val <- list()
for(i in 1:nrow(review_sentiment)){
  if(review_sentiment$positive[i] > review_sentiment$negative[i]){
    val <- c(val, 1)
  }else{
    val <- c(val, 0)
  }
}
```

```{r}
print(Reduce(`+`, val))
print(length(val))
```

most of the values were positive

```{r}
class(val)
val <- data.frame(matrix(unlist(val), nrow=length(val), byrow=TRUE))
df_total$is_positiv <- val
```

check if it added correctly or not which from the value below it added correctly

```{r}
print(sum(df_total$is_positiv))
```

```{r}
inaug_dfm_df <- as.data.frame(inaug_dfm)
class(df_total$`df$bool_HIGH`)
class(df_total$is_positiv)
df_final <- cbind(inaug_dfm_df, df_total$is_positiv, df_total$`df$bool_HIGH`)
X <- select(df_final, -target, -doc_id)
y <- select(df_final, c(16598))
colnames(df_final)[16598] <- "target" 
colnames(df_final)[16597] <- "is_positive"
ncol(inaug_dfm_df)
ncol(df_final)
nrow(x_train)
nrow(y_train)
nrow(x_test)
nrow(y_test)
class(x_test)
class(x_test)
```

```{r}
x_train <- X[1:6000,]
y_train <- y[1:6000,]
  
x_test <- X[6001:7566,]
y_test <- y[6001:7566,]
```

```{r}
install.packages("class")
install.packages("gmodels")
library(class)
library(gmodels)
library(crosstable)
```

![]()

```{r}
y_pred <- knn(train = x_train, test = x_test,cl = y_train, k=10)``
```

```{r}
CrossTable(x= y_test, y=y_pred,prop.chisq=FALSE)
```

NEED OVERSAMPLING OR DESAMPLING BECAUSE IT IS NOT GIVING GREAT RESULTS

```{r}

```
