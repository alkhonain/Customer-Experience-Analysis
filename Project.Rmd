# **Project**

The competitiveness between coffee shops is increasing exponentially, and customer reviews usually determines the success of each café; so we decided to offer help by making a sentiment analysis of customer reviews, and getting each Café's needs based on multiple factors.
" NEED TO BE EDITED"

## Dataset

Our dataset is scraped from yelp.com containing nearly 7,000 reviews, about coffee shops in Austin and Texas. The attributes are replaced, and sentiments around attributes.
" NEED TO BE EDITED "

The columns we used:

-   Coffee shop name

-   Review text

-   Review Score

### Import

```{r}
#install.packages("SnowballC")
```

```{r message=FALSE}
library(reticulate)
library(ggplot2)
library(mlbench)
library(tidyverse)
library(psych)
library(dplyr)
library(tm)
library(tokenizers)
library(SnowballC)
```

```{r message=FALSE}
df <- read_csv("Dataset/ratings_and_sentiments.csv")
# Rows: 7621 Columns: 20  
df <-as.data.frame(df)
print(class(df))
head(df)[1, 1:2]

```

```{r warning=FALSE}
describe(df) # the start mean the value is object otherwise its numbercal
```

Removing missing values and checking where they occurred.

```{r}
sum(is.na(df)) # Checks whether the DS contains any missing values
sum(is.na(df$vibe_sent)) # vibe_sent column contains most of the missing values; so we will remove it.
df = select(df, -vibe_sent)
df <- na.omit(df)
sum(is.na(df)) 
```

Remove any Coffee Shop with less than 20 reviews.

```{r}
table(df['coffee_shop_name'])

df <- df[df$coffee_shop_name != 'Sister Coffee',]
df <- df[df$coffee_shop_name != 'Kowabunga Coffee',]
df <- df[df$coffee_shop_name != 'The Marvelous Vintage Tea Party Co.',]
df <- df[df$coffee_shop_name != 'Lola Savannah Coffee Downtown',]
```

```{r}
nrow(df)
```

Processing the reviews by cleaning them from any numbers, links, spaces, punctuation etc...

```{r}
# Get the text column
text <- df$review_text
# Removing mentions, URLs, Emojis, numbers, punctuation, etc.
text <- gsub("@\\w+", "", text)
text <- gsub("https?://.+", "", text)
text <- gsub("\\d+\\w*\\d*", "", text)
text <- gsub("#\\w+", "", text)
text <- gsub("[^\x01-\x7F]", "", text)
text <- gsub("[[:punct:]]", " ", text)
# Remove spaces and newlines
text <- gsub("\n", " ", text)
text <- gsub("^\\s+", "", text)
text <- gsub("\\s+$", "", text)
text <- gsub("[ |\t]+", " ", text)
# Set the text to lowercase
text <- tolower(text)
```

Using NLP techniques to remove stop words, tokenize, etc...

```{r warning=FALSE, message=FALSE}
documents = c("She had toast for breakfast",
              "The coffee this morning was excellent", 
              "For lunch let's all have pancakes")
documents <- Corpus(VectorSource(documents))
documents = tm_map(documents, content_transformer(tolower))
documents = tm_map(documents, removePunctuation)
documents = tm_map(documents, stripWhitespace)
documents = tm_map(documents, removeWords, stopwords("english"))
documents

documents[[1]]$content
documents[[2]]$content
documents[[3]]$content

```

Above is a sample of what kind of removal will be applied to the text, so if we implement that on the review_text it will be as shown below: " NEED TO BE EDITED "

```{r}
view(text)
```

```{r warning=FALSE}
documents <- Corpus(VectorSource(text))
documents = tm_map(documents, content_transformer(tolower))
documents = tm_map(documents, removePunctuation)
documents = tm_map(documents, removeWords, stopwords("english"))
documents

documents[[1]]$content
documents[[2]]$content
documents[[3]]$content

```

The text is better after we normalized it, but some words are not in their proper use cases; so we will check to see whether each word is in the English dictionary or not.

```{r}
documents[[1]]$content
tokenize_words(documents[[1]]$content)
```

The idea above shows how it splits each word individually (Tokenize), the next step is stemming and lemmatization.

Stemming Words - After tokenization, we need to analyze each word by reverting it to its base form (stemming) and conjugation affix. " NEED TO BE EDITED "
